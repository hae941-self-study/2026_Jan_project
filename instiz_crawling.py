"""
ì¸ìŠ¤í‹°ì¦ˆ (Instiz) íŠ¸ë Œë“œ í¬ë¡¤ë§
- ì¸ê¸° ê²Œì‹œë¬¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
- ì‹¤ì‹œê°„ ì´ìŠˆ ë¶„ì„
"""

import requests
from bs4 import BeautifulSoup
import time
import random
import re
from collections import Counter
from typing import List, Dict
import json
import csv
from datetime import datetime
import sys
import io

# Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')


class InstizCrawler:
    """ì¸ìŠ¤í‹°ì¦ˆ í¬ë¡¤ëŸ¬"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        self.base_url = 'https://www.instiz.net'

    def get_ichart_trends(self, max_items: int = 50) -> List[Dict]:
        """
        ì¸ìŠ¤í‹°ì¦ˆ ì•„ì´ì°¨íŠ¸ (ì‹¤ì‹œê°„ ì°¨íŠ¸) ê°€ì ¸ì˜¤ê¸°

        Args:
            max_items: ìˆ˜ì§‘í•  í•­ëª© ìˆ˜

        Returns:
            ì°¨íŠ¸ í•­ëª© ë¦¬ìŠ¤íŠ¸
        """
        # ì—¬ëŸ¬ URL ì‹œë„
        urls_to_try = [
            f'{self.base_url}/pt',  # ì „ì²´ ê²Œì‹œíŒ
            f'{self.base_url}/pt/0',  # ì¸ê¸°ê¸€
            'https://www.instiz.net/name',  # ë„¤ì„ë“œ
        ]

        for url in urls_to_try:
            try:
                print(f"   ì‹œë„ ì¤‘: {url}")
                response = requests.get(url, headers=self.headers, timeout=10)
                response.raise_for_status()
                response.encoding = 'utf-8'

                soup = BeautifulSoup(response.text, 'html.parser')
                items = []

                # ì—¬ëŸ¬ ì„ íƒì ì‹œë„
                selectors = [
                    '.postBtn',
                    '.post-list-item',
                    'tr.tr',
                    '.list-item',
                    'a[class*="subject"]',
                    'td.subject',
                    '.sbj'
                ]

                posts = []
                for selector in selectors:
                    posts = soup.select(selector)
                    if posts:
                        print(f"   âœ“ ì„ íƒì '{selector}' ë°œê²¬: {len(posts)}ê°œ")
                        break

                if not posts:
                    print(f"   âœ— ê²Œì‹œë¬¼ì„ ì°¾ì§€ ëª»í•¨")
                    # ë””ë²„ê·¸: HTML ì¼ë¶€ ì¶œë ¥
                    # print(f"   HTML ìƒ˜í”Œ: {soup.prettify()[:500]}")
                    continue

                for post in posts[:max_items]:
                    try:
                        # ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì œëª© ì¶”ì¶œ ì‹œë„
                        title = None

                        # ë°©ë²• 1: .title í´ë˜ìŠ¤
                        title_elem = post.select_one('.title')
                        if title_elem:
                            title = title_elem.text.strip()

                        # ë°©ë²• 2: a íƒœê·¸
                        if not title:
                            title_elem = post.select_one('a')
                            if title_elem:
                                title = title_elem.get('title', '') or title_elem.text.strip()

                        # ë°©ë²• 3: ì§ì ‘ í…ìŠ¤íŠ¸
                        if not title:
                            title = post.text.strip()

                        if not title or len(title) < 2:
                            continue

                        # ëŒ“ê¸€ ìˆ˜
                        comment_elem = post.select_one('.cmtnum') or post.select_one('[class*="cmt"]')
                        comments = 0
                        if comment_elem:
                            comment_text = comment_elem.text.strip()
                            comment_match = re.search(r'(\d+)', comment_text)
                            if comment_match:
                                comments = int(comment_match.group(1))

                        items.append({
                            'title': title,
                            'comments': comments,
                            'engagement': comments + 1
                        })

                    except Exception as e:
                        continue

                if items:
                    print(f"   âœ“ {len(items)}ê°œ ê²Œì‹œë¬¼ ìˆ˜ì§‘ ì„±ê³µ")
                    return items

            except requests.exceptions.RequestException as e:
                print(f"   âœ— ì‹¤íŒ¨: {e}")
                continue

        print(f"âŒ ëª¨ë“  URLì—ì„œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
        return []

    def get_board_posts(self, board_id: str, max_pages: int = 5) -> List[Dict]:
        """
        íŠ¹ì • ê²Œì‹œíŒì˜ ê²Œì‹œë¬¼ ê°€ì ¸ì˜¤ê¸°

        Args:
            board_id: ê²Œì‹œíŒ ID
            max_pages: í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜

        Returns:
            ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸
        """
        posts = []

        for page in range(1, max_pages + 1):
            url = f'{self.base_url}/bbs/{board_id}?page={page}'

            try:
                response = requests.get(url, headers=self.headers, timeout=10)
                response.raise_for_status()
                response.encoding = 'utf-8'

                soup = BeautifulSoup(response.text, 'html.parser')

                # ê²Œì‹œë¬¼ ëª©ë¡ íŒŒì‹±
                post_list = soup.select('.postBtn')

                for post in post_list:
                    try:
                        # ì œëª©
                        title_elem = post.select_one('.title')
                        if not title_elem:
                            continue

                        title = title_elem.text.strip()

                        # ëŒ“ê¸€ ìˆ˜
                        comment_elem = post.select_one('.cmtnum')
                        comments = 0
                        if comment_elem:
                            comment_text = comment_elem.text.strip()
                            comment_match = re.search(r'(\d+)', comment_text)
                            if comment_match:
                                comments = int(comment_match.group(1))

                        posts.append({
                            'title': title,
                            'comments': comments,
                            'engagement': comments
                        })

                    except Exception as e:
                        continue

                print(f"   í˜ì´ì§€ {page}/{max_pages}: {len(post_list)}ê°œ ê²Œì‹œë¬¼ ìˆ˜ì§‘")

                # Rate limit ë°©ì§€
                time.sleep(random.uniform(1, 2))

            except Exception as e:
                print(f"   âš ï¸ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                continue

        return posts

    def extract_keywords_from_posts(self, posts: List[Dict],
                                   min_length: int = 2) -> List[Dict]:
        """
        ê²Œì‹œë¬¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ

        Args:
            posts: ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸
            min_length: ìµœì†Œ í‚¤ì›Œë“œ ê¸¸ì´

        Returns:
            í‚¤ì›Œë“œì™€ ë¹ˆë„ìˆ˜
        """
        keyword_counter = Counter()
        keyword_engagement = {}

        # ë¶ˆìš©ì–´
        stopwords = {
            'ì¸ìŠ¤í‹°ì¦ˆ', 'ê²Œì‹œíŒ', 'ê²Œì‹œê¸€', 'ê³µì§€', 'ì§ˆë¬¸', 'ë‹µë³€',
            'ì…ë‹ˆë‹¤', 'í•©ë‹ˆë‹¤', 'ìˆìŠµë‹ˆë‹¤', 'ì—†ìŠµë‹ˆë‹¤', 'ê°€ëŠ¥', 'ë¶ˆê°€ëŠ¥',
            'ì´ê±°', 'ì €ê±°', 'ê·¸ê±°', 'ì´ê²Œ', 'ì €ê²Œ', 'ê·¸ê²Œ',
            'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ìš”ì¦˜', 'ì§€ê¸ˆ', 'ì´ì œ', 'ê·¸ëƒ¥',
            'ì§„ì§œ', 'ì •ë§', 'ì™„ì „', 'ë„ˆë¬´', 'ì—„ì²­', 'ê°œ', 'ë§¤ìš°',
            'ìˆë‹¤', 'ì—†ë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ì´ë‹¤', 'ì•„ë‹ˆë‹¤',
            'ê°™ë‹¤', 'ë“¯í•˜ë‹¤', 'ë³´ì´ë‹¤', 'ì‹¶ë‹¤', 'í•˜ê³ ', 'ê·¸ë¦¬ê³ ',
            'ë˜ëŠ”', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ê·¸ë˜ì„œ', 'ë•Œë¬¸ì—',
            'ì•ˆë…•í•˜ì„¸ìš”', 'ê°ì‚¬í•©ë‹ˆë‹¤', 'ìˆ˜ê³ í•˜ì„¸ìš”', 'ë¶€íƒë“œë¦½ë‹ˆë‹¤'
        }

        for post in posts:
            title = post['title']
            engagement = post.get('engagement', 1)

            # í•œê¸€ í‚¤ì›Œë“œ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
            korean_words = re.findall(r'[ê°€-í£]{2,}', title)

            # ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ (2ê¸€ì ì´ìƒ, ëŒ€ì†Œë¬¸ì êµ¬ë¶„)
            english_words = re.findall(r'\b[A-Z][a-z]+\b|\b[A-Z]{2,}\b', title)

            # ìˆ«ì+í…ìŠ¤íŠ¸ ì¡°í•©
            mixed_words = re.findall(r'\d+[ê°€-í£]+', title)

            all_words = korean_words + english_words + mixed_words

            for word in all_words:
                # ë¶ˆìš©ì–´ ì œê±°
                if word.lower() in stopwords or len(word) < min_length:
                    continue

                keyword_counter[word] += 1

                # ì¸ê¸°ë„ ëˆ„ì 
                if word not in keyword_engagement:
                    keyword_engagement[word] = 0
                keyword_engagement[word] += engagement

        # ê²°ê³¼ ì •ë¦¬
        keywords = []
        for word, count in keyword_counter.most_common(100):
            keywords.append({
                'keyword': word,
                'count': count,
                'total_engagement': keyword_engagement.get(word, 0),
                'avg_engagement': keyword_engagement.get(word, 0) / count if count > 0 else 0
            })

        return keywords


class InstizTrendAnalyzer:
    """ì¸ìŠ¤í‹°ì¦ˆ íŠ¸ë Œë“œ ë¶„ì„ê¸°"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.crawler = InstizCrawler()

    def analyze_ichart(self, max_items: int = 100) -> Dict:
        """
        ì•„ì´ì°¨íŠ¸ ë¶„ì„

        Args:
            max_items: ìˆ˜ì§‘í•  í•­ëª© ìˆ˜

        Returns:
            ë¶„ì„ ê²°ê³¼
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“Š ì¸ìŠ¤í‹°ì¦ˆ ì‹¤ì‹œê°„ ì¸ê¸°ê¸€ ë¶„ì„")
        print(f"{'='*60}")

        items = self.crawler.get_ichart_trends(max_items)

        if not items:
            print(f"âš ï¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return {}

        print(f"ğŸ“Š ì´ {len(items)}ê°œ ê²Œì‹œë¬¼ ìˆ˜ì§‘ ì™„ë£Œ")

        # í‚¤ì›Œë“œ ì¶”ì¶œ
        print(f"ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        keywords = self.crawler.extract_keywords_from_posts(items)

        print(f"âœ… {len(keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ")

        return {
            'source': 'ì¸ìŠ¤í‹°ì¦ˆ ì‹¤ì‹œê°„ ì¸ê¸°ê¸€',
            'total_posts': len(items),
            'keywords': keywords,
            'crawled_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

    def analyze_boards(self, boards: List[Dict], max_pages: int = 5) -> Dict:
        """
        ì—¬ëŸ¬ ê²Œì‹œíŒ ë¶„ì„

        Args:
            boards: [{'id': 'board_id', 'name': 'board_name'}, ...]
            max_pages: ê²Œì‹œíŒë‹¹ í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜

        Returns:
            ì „ì²´ ë¶„ì„ ê²°ê³¼
        """
        results = {}

        for board in boards:
            board_id = board['id']
            board_name = board['name']

            print(f"\n{'='*60}")
            print(f"ğŸ“± {board_name} í¬ë¡¤ë§ ì¤‘...")
            print(f"{'='*60}")

            posts = self.crawler.get_board_posts(board_id, max_pages)

            if not posts:
                print(f"âš ï¸ {board_name}: ê²Œì‹œë¬¼ì„ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                continue

            print(f"ğŸ“Š ì´ {len(posts)}ê°œ ê²Œì‹œë¬¼ ìˆ˜ì§‘ ì™„ë£Œ")

            # í‚¤ì›Œë“œ ì¶”ì¶œ
            print(f"ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
            keywords = self.crawler.extract_keywords_from_posts(posts)

            print(f"âœ… {len(keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ")

            results[board_id] = {
                'board_name': board_name,
                'total_posts': len(posts),
                'keywords': keywords,
                'crawled_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }

            # ê²Œì‹œíŒë³„ Top 10 ì¶œë ¥
            print(f"\nğŸ† {board_name} Top 10 í‚¤ì›Œë“œ:")
            print("-" * 70)
            for i, kw in enumerate(keywords[:10], 1):
                print(f"{i:2d}. {kw['keyword']:20s} | "
                      f"ì¶œí˜„: {kw['count']:3d}íšŒ | "
                      f"ì¸ê¸°ë„: {kw['total_engagement']:5d}")

            # ê²Œì‹œíŒ ì‚¬ì´ ëŒ€ê¸°
            time.sleep(random.uniform(2, 3))

        return results

    def save_results(self, results: Dict, filename: str = 'instiz_trends.json'):
        """ê²°ê³¼ ì €ì¥ (JSON)"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def save_results_to_csv(self, results: Dict, filename: str = 'instiz_trends.csv'):
        """ê²°ê³¼ ì €ì¥ (CSV)"""
        with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['ì¶œì²˜', 'ìˆœìœ„', 'í‚¤ì›Œë“œ', 'ì¶œí˜„íšŸìˆ˜', 'ì´_ì¸ê¸°ë„', 'í‰ê· _ì¸ê¸°ë„'])

            for key, result in results.items():
                source_name = result.get('board_name', result.get('source', key))
                for i, kw in enumerate(result['keywords'], 1):
                    writer.writerow([
                        source_name,
                        i,
                        kw['keyword'],
                        kw['count'],
                        kw.get('total_engagement', 0),
                        round(kw.get('avg_engagement', 0), 2)
                    ])

        print(f"ğŸ’¾ CSV ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# ì‹¤í–‰
if __name__ == "__main__":
    print("\n" + "="*80)
    print("ğŸš€ ì¸ìŠ¤í‹°ì¦ˆ íŠ¸ë Œë“œ í¬ë¡¤ë§")
    print("="*80)
    print("\nğŸ“Š ì¸ìŠ¤í‹°ì¦ˆì—ì„œ ì‹¤ì‹œê°„ íŠ¸ë Œë“œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    print("âš ï¸  í¬ë¡¤ë§ ì†ë„ ì œí•œì„ ì¤€ìˆ˜í•˜ë©°, ê³µê°œ ê²Œì‹œíŒë§Œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n")

    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = InstizTrendAnalyzer()

    try:
        # ì‹¤ì‹œê°„ ì¸ê¸°ê¸€ ë¶„ì„
        result = analyzer.analyze_ichart(max_items=100)

        if result:
            # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜ (ì €ì¥ìš©)
            results = {'ichart': result}

            # ê²°ê³¼ ì €ì¥
            analyzer.save_results(results, 'instiz_trends_2025.json')
            analyzer.save_results_to_csv(results, 'instiz_trends_2025.csv')

            # Top 20 í‚¤ì›Œë“œ ì¶œë ¥
            print("\n" + "="*80)
            print("ğŸ“Š ì¸ìŠ¤í‹°ì¦ˆ ì‹¤ì‹œê°„ íŠ¸ë Œë“œ Top 20")
            print("="*80)

            keywords = result['keywords'][:20]
            for i, kw in enumerate(keywords, 1):
                print(f"{i:2d}. {kw['keyword']:20s} | "
                      f"ì¶œí˜„: {kw['count']:3d}íšŒ | "
                      f"ì¸ê¸°ë„: {kw['total_engagement']:5d}")

            print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ!")
            print(f"ğŸ“… ìˆ˜ì§‘ ì‹œê°„: {result['crawled_at']}")

        else:
            print("\nâŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

"""
í´ë¦¬ì•™ (Clien) íŠ¸ë Œë“œ í¬ë¡¤ë§
- ì¸ê¸° ê²Œì‹œë¬¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
- ì›”ê°„ ë² ìŠ¤íŠ¸ ê²Œì‹œíŒ ì§€ì›
"""

import requests
from bs4 import BeautifulSoup
import time
import random
import re
from collections import Counter
from typing import List, Dict
import json
import csv
from datetime import datetime
import sys
import io

# Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')


class ClienCrawler:
    """í´ë¦¬ì•™ í¬ë¡¤ëŸ¬"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        self.base_url = 'https://www.clien.net'

    def get_board_posts(self, board_type: str = 'park', max_pages: int = 5) -> List[Dict]:
        """
        ê²Œì‹œíŒì˜ ê²Œì‹œë¬¼ ê°€ì ¸ì˜¤ê¸°

        Args:
            board_type: ê²Œì‹œíŒ íƒ€ì…
                - 'park': ëª¨ë‘ì˜ê³µì› (ììœ ê²Œì‹œíŒ)
                - 'jirum': ì•Œëœ°êµ¬ë§¤
                - 'cm_car': ìë™ì°¨
                - 'cm_vcam': ì˜ìƒê¸°ê¸°
                - 'lecture': ê°•ì¢Œ
            max_pages: í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜

        Returns:
            ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸
        """
        posts = []

        for page in range(0, max_pages):
            url = f'{self.base_url}/service/board/{board_type}'

            params = {
                'od': 'T31',  # ì¸ê¸°ìˆœ
                'po': page * 15  # 15ê°œì”© í˜ì´ì§€ë„¤ì´ì…˜
            }

            try:
                response = requests.get(url, params=params, headers=self.headers, timeout=10)
                response.raise_for_status()
                response.encoding = 'utf-8'

                soup = BeautifulSoup(response.text, 'html.parser')

                # ê²Œì‹œë¬¼ ëª©ë¡ íŒŒì‹±
                post_list = soup.select('.list_item')

                for post in post_list:
                    try:
                        # ì œëª©
                        title_elem = post.select_one('.subject_fixed')
                        if not title_elem:
                            title_elem = post.select_one('.list_subject')

                        if not title_elem:
                            continue

                        title = title_elem.text.strip()

                        # ëŒ“ê¸€ ìˆ˜
                        comment_elem = post.select_one('.comment_count')
                        comments = 0
                        if comment_elem:
                            comment_text = comment_elem.text.strip()
                            comment_match = re.search(r'\[(\d+)\]', comment_text)
                            if comment_match:
                                comments = int(comment_match.group(1))

                        # ì¡°íšŒìˆ˜
                        hit_elem = post.select_one('.hit')
                        hits = 0
                        if hit_elem:
                            hit_text = hit_elem.text.strip()
                            hits = int(hit_text) if hit_text.isdigit() else 0

                        # ì¶”ì²œìˆ˜
                        symph_elem = post.select_one('.symph_count')
                        symphs = 0
                        if symph_elem:
                            symph_text = symph_elem.text.strip()
                            symphs = int(symph_text) if symph_text.isdigit() else 0

                        posts.append({
                            'title': title,
                            'comments': comments,
                            'hits': hits,
                            'symphs': symphs,
                            'engagement': comments * 5 + symphs * 10  # ê°€ì¤‘ì¹˜
                        })

                    except Exception as e:
                        continue

                if len(posts) > 0:
                    print(f"   âœ… í˜„ì¬ê¹Œì§€ ì´ {len(posts)}ê°œ ê²Œì‹œë¬¼ ìˆ˜ì§‘")
                else:
                    print(f"   âš ï¸ ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ ì—†ìŒ")

                # Rate limit ë°©ì§€
                time.sleep(random.uniform(1, 2))

            except requests.exceptions.HTTPError as e:
                print(f"   âš ï¸ HTTP ì—ëŸ¬: {e}")
                continue
            except Exception as e:
                print(f"   âš ï¸ í˜ì´ì§€ {page + 1} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                continue

        print(f"\nğŸ“Š ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(posts)}ê°œ ê²Œì‹œë¬¼")
        return posts

    def get_monthly_best(self, max_pages: int = 10) -> List[Dict]:
        """
        ì›”ê°„ ë² ìŠ¤íŠ¸ ê²Œì‹œíŒ ê°€ì ¸ì˜¤ê¸° (ëª¨ë‘ì˜ê³µì› ì¸ê¸°ê¸€)

        Args:
            max_pages: í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜

        Returns:
            ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸
        """
        posts = []

        # ëª¨ë‘ì˜ê³µì›(park) ê²Œì‹œíŒì˜ ì¸ê¸°ê¸€ë¡œ ë³€ê²½
        for page in range(0, max_pages):
            url = f'{self.base_url}/service/board/park'

            params = {
                'od': 'T31',  # ì¸ê¸°ìˆœ
                'po': page * 15
            }

            try:
                print(f"   í˜ì´ì§€ {page + 1}/{max_pages} ìš”ì²­ ì¤‘: {url}")
                response = requests.get(url, params=params, headers=self.headers, timeout=10)

                print(f"   ì‘ë‹µ ì½”ë“œ: {response.status_code}")

                if response.status_code == 404:
                    print(f"   âš ï¸ 404 ì—ëŸ¬ - URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    continue

                response.raise_for_status()
                response.encoding = 'utf-8'

                soup = BeautifulSoup(response.text, 'html.parser')

                # ê²Œì‹œë¬¼ ëª©ë¡ íŒŒì‹± (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)
                post_list = soup.select('.list_item')

                if not post_list:
                    post_list = soup.select('div[class*="list"]')

                if not post_list:
                    print(f"   âš ï¸ ê²Œì‹œë¬¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    continue

                print(f"   âœ“ {len(post_list)}ê°œ ê²Œì‹œë¬¼ ë°œê²¬")

                for post in post_list:
                    try:
                        # ì œëª©
                        title_elem = post.select_one('.subject_fixed') or post.select_one('.list_subject')
                        if not title_elem:
                            continue

                        title = title_elem.text.strip()

                        # ëŒ“ê¸€ ìˆ˜
                        comment_elem = post.select_one('.comment_count')
                        comments = 0
                        if comment_elem:
                            comment_text = comment_elem.text.strip()
                            comment_match = re.search(r'\[(\d+)\]', comment_text)
                            if comment_match:
                                comments = int(comment_match.group(1))

                        # ì¶”ì²œìˆ˜
                        symph_elem = post.select_one('.symph_count')
                        symphs = 0
                        if symph_elem:
                            symph_text = symph_elem.text.strip()
                            symphs = int(symph_text) if symph_text.isdigit() else 0

                        posts.append({
                            'title': title,
                            'comments': comments,
                            'symphs': symphs,
                            'engagement': comments * 5 + symphs * 10
                        })

                    except Exception as e:
                        continue

                if len(posts) > 0:
                    print(f"   âœ… í˜„ì¬ê¹Œì§€ ì´ {len(posts)}ê°œ ê²Œì‹œë¬¼ ìˆ˜ì§‘")
                else:
                    print(f"   âš ï¸ ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ ì—†ìŒ")

                # Rate limit ë°©ì§€
                time.sleep(random.uniform(1, 2))

            except requests.exceptions.HTTPError as e:
                print(f"   âš ï¸ HTTP ì—ëŸ¬: {e}")
                continue
            except Exception as e:
                print(f"   âš ï¸ í˜ì´ì§€ {page + 1} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                continue

        print(f"\nğŸ“Š ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(posts)}ê°œ ê²Œì‹œë¬¼")
        return posts

    def extract_keywords_from_posts(self, posts: List[Dict],
                                   min_length: int = 2) -> List[Dict]:
        """
        ê²Œì‹œë¬¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ

        Args:
            posts: ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸
            min_length: ìµœì†Œ í‚¤ì›Œë“œ ê¸¸ì´

        Returns:
            í‚¤ì›Œë“œì™€ ë¹ˆë„ìˆ˜
        """
        keyword_counter = Counter()
        keyword_engagement = {}

        # ë¶ˆìš©ì–´
        stopwords = {
            'í´ë¦¬ì•™', 'ê²Œì‹œíŒ', 'ê²Œì‹œê¸€', 'ê³µì§€', 'ì§ˆë¬¸', 'ë‹µë³€',
            'ì…ë‹ˆë‹¤', 'í•©ë‹ˆë‹¤', 'ìˆìŠµë‹ˆë‹¤', 'ì—†ìŠµë‹ˆë‹¤', 'ê°€ëŠ¥', 'ë¶ˆê°€ëŠ¥',
            'ì´ê±°', 'ì €ê±°', 'ê·¸ê±°', 'ì´ê²Œ', 'ì €ê²Œ', 'ê·¸ê²Œ',
            'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ìš”ì¦˜', 'ì§€ê¸ˆ', 'ì´ì œ', 'ê·¸ëƒ¥',
            'ì§„ì§œ', 'ì •ë§', 'ì™„ì „', 'ë„ˆë¬´', 'ì—„ì²­', 'ê°œ', 'ë§¤ìš°',
            'ìˆë‹¤', 'ì—†ë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ì´ë‹¤', 'ì•„ë‹ˆë‹¤',
            'ê°™ë‹¤', 'ë“¯í•˜ë‹¤', 'ë³´ì´ë‹¤', 'ì‹¶ë‹¤', 'í•˜ê³ ', 'ê·¸ë¦¬ê³ ',
            'ë˜ëŠ”', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ê·¸ë˜ì„œ', 'ë•Œë¬¸ì—',
            'ì•ˆë…•í•˜ì„¸ìš”', 'ê°ì‚¬í•©ë‹ˆë‹¤', 'ìˆ˜ê³ í•˜ì„¸ìš”', 'ë¶€íƒë“œë¦½ë‹ˆë‹¤',
            'ëª¨ë‘ì˜ê³µì›', 'ì•Œëœ°êµ¬ë§¤', 'ìë™ì°¨', 'ì˜ìƒê¸°ê¸°'
        }

        for post in posts:
            title = post['title']
            engagement = post.get('engagement', 1)

            # í•œê¸€ í‚¤ì›Œë“œ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
            korean_words = re.findall(r'[ê°€-í£]{2,}', title)

            # ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
            english_words = re.findall(r'\b[A-Za-z]{2,}\b', title)

            # ìˆ«ì+í…ìŠ¤íŠ¸ ì¡°í•©
            mixed_words = re.findall(r'\d+[ê°€-í£]+', title)

            all_words = korean_words + english_words + mixed_words

            for word in all_words:
                # ë¶ˆìš©ì–´ ì œê±°
                if word.lower() in stopwords or len(word) < min_length:
                    continue

                keyword_counter[word] += 1

                # ì¸ê¸°ë„ ëˆ„ì 
                if word not in keyword_engagement:
                    keyword_engagement[word] = 0
                keyword_engagement[word] += engagement

        # ê²°ê³¼ ì •ë¦¬
        keywords = []
        for word, count in keyword_counter.most_common(100):
            keywords.append({
                'keyword': word,
                'count': count,
                'total_engagement': keyword_engagement.get(word, 0),
                'avg_engagement': keyword_engagement.get(word, 0) / count if count > 0 else 0
            })

        return keywords


class ClienTrendAnalyzer:
    """í´ë¦¬ì•™ íŠ¸ë Œë“œ ë¶„ì„ê¸°"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.crawler = ClienCrawler()

    def analyze_boards(self, boards: List[Dict], max_pages: int = 5) -> Dict:
        """
        ì—¬ëŸ¬ ê²Œì‹œíŒ ë¶„ì„

        Args:
            boards: [{'type': 'board_type', 'name': 'board_name'}, ...]
            max_pages: ê²Œì‹œíŒë‹¹ í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜

        Returns:
            ì „ì²´ ë¶„ì„ ê²°ê³¼
        """
        results = {}

        for board in boards:
            board_type = board['type']
            board_name = board['name']

            print(f"\n{'='*60}")
            print(f"ğŸ“± {board_name} í¬ë¡¤ë§ ì¤‘...")
            print(f"{'='*60}")

            posts = self.crawler.get_board_posts(board_type, max_pages)

            if not posts:
                print(f"âš ï¸ {board_name}: ê²Œì‹œë¬¼ì„ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                continue

            print(f"ğŸ“Š ì´ {len(posts)}ê°œ ê²Œì‹œë¬¼ ìˆ˜ì§‘ ì™„ë£Œ")

            # í‚¤ì›Œë“œ ì¶”ì¶œ
            print(f"ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
            keywords = self.crawler.extract_keywords_from_posts(posts)

            print(f"âœ… {len(keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ")

            results[board_type] = {
                'board_name': board_name,
                'total_posts': len(posts),
                'keywords': keywords,
                'crawled_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }

            # ê²Œì‹œíŒë³„ Top 10 ì¶œë ¥
            print(f"\nğŸ† {board_name} Top 10 í‚¤ì›Œë“œ:")
            print("-" * 70)
            for i, kw in enumerate(keywords[:10], 1):
                print(f"{i:2d}. {kw['keyword']:20s} | "
                      f"ì¶œí˜„: {kw['count']:3d}íšŒ | "
                      f"ì¸ê¸°ë„: {kw['total_engagement']:6d}")

            # ê²Œì‹œíŒ ì‚¬ì´ ëŒ€ê¸°
            time.sleep(random.uniform(2, 3))

        return results

    def analyze_monthly_best(self, max_pages: int = 10) -> Dict:
        """
        ì›”ê°„ ë² ìŠ¤íŠ¸ ë¶„ì„

        Args:
            max_pages: í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜

        Returns:
            ë¶„ì„ ê²°ê³¼
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“Š í´ë¦¬ì•™ ì›”ê°„ ë² ìŠ¤íŠ¸ ë¶„ì„")
        print(f"{'='*60}")

        posts = self.crawler.get_monthly_best(max_pages)

        if not posts:
            print(f"âš ï¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return {}

        print(f"ğŸ“Š ì´ {len(posts)}ê°œ ê²Œì‹œë¬¼ ìˆ˜ì§‘ ì™„ë£Œ")

        # í‚¤ì›Œë“œ ì¶”ì¶œ
        print(f"ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        keywords = self.crawler.extract_keywords_from_posts(posts)

        print(f"âœ… {len(keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ")

        return {
            'source': 'í´ë¦¬ì•™ ì›”ê°„ ë² ìŠ¤íŠ¸',
            'total_posts': len(posts),
            'keywords': keywords,
            'crawled_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

    def save_results(self, results: Dict, filename: str = 'clien_trends.json'):
        """ê²°ê³¼ ì €ì¥ (JSON)"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def save_results_to_csv(self, results: Dict, filename: str = 'clien_trends.csv'):
        """ê²°ê³¼ ì €ì¥ (CSV)"""
        with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['ê²Œì‹œíŒ', 'ìˆœìœ„', 'í‚¤ì›Œë“œ', 'ì¶œí˜„íšŸìˆ˜', 'ì´_ì¸ê¸°ë„', 'í‰ê· _ì¸ê¸°ë„'])

            for key, result in results.items():
                source_name = result.get('board_name', result.get('source', key))
                for i, kw in enumerate(result['keywords'], 1):
                    writer.writerow([
                        source_name,
                        i,
                        kw['keyword'],
                        kw['count'],
                        kw.get('total_engagement', 0),
                        round(kw.get('avg_engagement', 0), 2)
                    ])

        print(f"ğŸ’¾ CSV ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# ì‹¤í–‰
if __name__ == "__main__":
    print("\n" + "="*80)
    print("ğŸš€ í´ë¦¬ì•™ íŠ¸ë Œë“œ í¬ë¡¤ë§")
    print("="*80)
    print("\nğŸ“± í´ë¦¬ì•™ì—ì„œ íŠ¸ë Œë“œ í‚¤ì›Œë“œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    print("âš ï¸  í¬ë¡¤ë§ ì†ë„ ì œí•œì„ ì¤€ìˆ˜í•˜ë©°, ê³µê°œ ê²Œì‹œíŒë§Œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n")

    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = ClienTrendAnalyzer()

    try:
        # ===== ì˜µì…˜ 1: ì›”ê°„ ë² ìŠ¤íŠ¸ ë¶„ì„ (ì¶”ì²œ!) =====
        print("âœ… ì›”ê°„ ë² ìŠ¤íŠ¸ ê²Œì‹œíŒ ë¶„ì„ ëª¨ë“œ")

        result = analyzer.analyze_monthly_best(max_pages=10)

        if result:
            results = {'monthly_best': result}

            # ê²°ê³¼ ì €ì¥
            analyzer.save_results(results, 'clien_trends_2025.json')
            analyzer.save_results_to_csv(results, 'clien_trends_2025.csv')

            # Top 20 í‚¤ì›Œë“œ ì¶œë ¥
            print("\n" + "="*80)
            print("ğŸ“Š í´ë¦¬ì•™ ì›”ê°„ ë² ìŠ¤íŠ¸ Top 20 í‚¤ì›Œë“œ")
            print("="*80)

            keywords = result['keywords'][:20]
            for i, kw in enumerate(keywords, 1):
                print(f"{i:2d}. {kw['keyword']:20s} | "
                      f"ì¶œí˜„: {kw['count']:3d}íšŒ | "
                      f"ì¸ê¸°ë„: {kw['total_engagement']:6d}")

            print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ!")
            print(f"ğŸ“… ìˆ˜ì§‘ ì‹œê°„: {result['crawled_at']}")

        # ===== ì˜µì…˜ 2: ì—¬ëŸ¬ ê²Œì‹œíŒ ë¶„ì„ =====
        # ì—¬ëŸ¬ ê²Œì‹œíŒì„ ë¶„ì„í•˜ë ¤ë©´ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”
        """
        boards = [
            {'type': 'park', 'name': 'ëª¨ë‘ì˜ê³µì›'},
            {'type': 'jirum', 'name': 'ì•Œëœ°êµ¬ë§¤'},
            {'type': 'cm_car', 'name': 'ìë™ì°¨'},
        ]

        results = analyzer.analyze_boards(boards, max_pages=5)

        if results:
            analyzer.save_results(results, 'clien_trends_2025.json')
            analyzer.save_results_to_csv(results, 'clien_trends_2025.csv')
        """

        if not result:
            print("\nâŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

"""
ë½ë¿Œ (PPOMPPU) íŠ¸ë Œë“œ í¬ë¡¤ë§
- í•«ë”œ/ì‡¼í•‘ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ì¶œ
- ë² ìŠ¤íŠ¸ ê²Œì‹œíŒ ì§€ì›
"""

import requests
from bs4 import BeautifulSoup
import time
import random
import re
from collections import Counter
from typing import List, Dict
import json
import csv
from datetime import datetime
import sys
import io

# Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')


class PpomppuCrawler:
    """ë½ë¿Œ í¬ë¡¤ëŸ¬"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        self.base_url = 'https://www.ppomppu.co.kr'

    def get_board_posts(self, board_id: str, max_pages: int = 5) -> List[Dict]:
        """
        ê²Œì‹œíŒì˜ ê²Œì‹œë¬¼ ê°€ì ¸ì˜¤ê¸°

        Args:
            board_id: ê²Œì‹œíŒ ID
                - 'zboard/zboard.php?id=ppomppu': ììœ ê²Œì‹œíŒ
                - 'zboard/zboard.php?id=ppomppu4': ìœ ë¨¸/ì´ìŠˆ
                - 'zboard/zboard.php?id=humor': ìœ ë¨¸ê²Œì‹œíŒ
                - 'zboard/zboard.php?id=freeboard': ììœ ê²Œì‹œíŒ
            max_pages: í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜

        Returns:
            ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸
        """
        posts = []

        for page in range(1, max_pages + 1):
            url = f'{self.base_url}/{board_id}&page={page}'

            try:
                print(f"   í˜ì´ì§€ {page}/{max_pages} ìš”ì²­ ì¤‘...")
                response = requests.get(url, headers=self.headers, timeout=10)
                response.raise_for_status()
                response.encoding = 'euc-kr'  # ë½ë¿ŒëŠ” euc-kr ì¸ì½”ë”©

                soup = BeautifulSoup(response.text, 'html.parser')

                # ê²Œì‹œë¬¼ ëª©ë¡ íŒŒì‹±
                post_list = soup.select('tr[class*="list"]')

                if not post_list:
                    post_list = soup.select('table.board_table tr')

                print(f"   âœ“ {len(post_list)}ê°œ í•­ëª© ë°œê²¬")

                for post in post_list:
                    try:
                        # ì œëª©
                        title_elem = post.select_one('a[class*="list_title"]') or post.select_one('td.list_title a')
                        if not title_elem:
                            title_elem = post.select_one('a.title') or post.select_one('td a')

                        if not title_elem:
                            continue

                        title = title_elem.text.strip()

                        # ê³µì§€ì‚¬í•­ ì œì™¸
                        if 'ê³µì§€' in title or 'ì•Œë¦¼' in title:
                            continue

                        # ì¡°íšŒìˆ˜
                        hit_elem = post.select_one('td.hit') or post.select_one('td[class*="hit"]')
                        hits = 0
                        if hit_elem:
                            hit_text = hit_elem.text.strip()
                            hits = int(hit_text) if hit_text.isdigit() else 0

                        # ì¶”ì²œìˆ˜
                        recommend_elem = post.select_one('td.recommend') or post.select_one('td[class*="rec"]')
                        recommends = 0
                        if recommend_elem:
                            rec_text = recommend_elem.text.strip()
                            recommends = int(rec_text) if rec_text.isdigit() else 0

                        posts.append({
                            'title': title,
                            'hits': hits,
                            'recommends': recommends,
                            'engagement': hits + recommends * 10
                        })

                    except Exception as e:
                        continue

                if len(posts) > 0:
                    print(f"   âœ… í˜„ì¬ê¹Œì§€ ì´ {len(posts)}ê°œ ê²Œì‹œë¬¼ ìˆ˜ì§‘")

                # Rate limit ë°©ì§€
                time.sleep(random.uniform(1, 2))

            except Exception as e:
                print(f"   âš ï¸ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                continue

        print(f"\nğŸ“Š ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(posts)}ê°œ ê²Œì‹œë¬¼")
        return posts

    def get_hotdeal_posts(self, max_pages: int = 10) -> List[Dict]:
        """
        í•«ë”œ ê²Œì‹œíŒ ê°€ì ¸ì˜¤ê¸°

        Args:
            max_pages: í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜

        Returns:
            ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸
        """
        posts = []

        for page in range(1, max_pages + 1):
            # ë½ë¿Œ í•«ë”œ ê²Œì‹œíŒ
            url = f'{self.base_url}/zboard/zboard.php?id=ppomppu&page={page}'

            try:
                print(f"   í˜ì´ì§€ {page}/{max_pages} ìš”ì²­ ì¤‘: {url}")
                response = requests.get(url, headers=self.headers, timeout=10)
                print(f"   ì‘ë‹µ ì½”ë“œ: {response.status_code}")
                response.raise_for_status()
                response.encoding = 'euc-kr'

                soup = BeautifulSoup(response.text, 'html.parser')

                # ê²Œì‹œíŒ í…Œì´ë¸” ì°¾ê¸°
                tables = soup.find_all('table')

                board_table = soup.find('table', {'class': 'board_list'}) or \
                             soup.find('table', {'class': 'list_table'}) or \
                             soup.find('table', id='revolution_main_table')

                if not board_table and tables:
                    # ê°€ì¥ í° í…Œì´ë¸” ì‚¬ìš©
                    board_table = max(tables, key=lambda t: len(str(t)))

                if not board_table:
                    print(f"   âš ï¸ ê²Œì‹œíŒ í…Œì´ë¸”ì„ ì°¾ì§€ ëª»í•¨")
                    continue

                # ê²Œì‹œë¬¼ í–‰ ì°¾ê¸°
                post_list = board_table.find_all('tr')

                # ìœ íš¨í•œ ê²Œì‹œë¬¼ë§Œ í•„í„°ë§ (ê³µë°± í–‰ ì œì™¸)
                valid_posts = [tr for tr in post_list if tr.find('td', class_='list_vspace') is None]

                if not valid_posts:
                    print(f"   âš ï¸ ìœ íš¨í•œ ê²Œì‹œë¬¼ì„ ì°¾ì§€ ëª»í•¨")
                    continue

                post_list = valid_posts

                successful_posts = 0
                for post in post_list:
                    try:
                        # ì œëª© ì°¾ê¸° - baseList-title í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ë§í¬
                        title_elem = post.find('a', class_='baseList-title')

                        if not title_elem or not title_elem.text.strip():
                            continue

                        title = title_elem.text.strip()

                        # ê³µì§€/ì•Œë¦¼ ì œì™¸
                        if any(word in title for word in ['ê³µì§€', 'ì•Œë¦¼', 'ê´‘ê³ ', 'ì´ë²¤íŠ¸', 'ì•ˆë‚´']):
                            continue

                        # ì¡°íšŒìˆ˜ - baseList-views í´ë˜ìŠ¤
                        hits = 0
                        hit_elem = post.find('td', class_='baseList-views')
                        if hit_elem:
                            hit_text = hit_elem.text.strip()
                            hits = int(hit_text) if hit_text.isdigit() else 0

                        # ì¶”ì²œìˆ˜ - baseList-rec í´ë˜ìŠ¤ (í˜•ì‹: "4 - 0")
                        recommends = 0
                        rec_elem = post.find('td', class_='baseList-rec')
                        if rec_elem:
                            rec_text = rec_elem.text.strip()
                            # "4 - 0" í˜•ì‹ì—ì„œ ì²« ë²ˆì§¸ ìˆ«ì ì¶”ì¶œ
                            import re
                            match = re.search(r'(\d+)', rec_text)
                            if match:
                                recommends = int(match.group(1))

                        posts.append({
                            'title': title,
                            'hits': hits,
                            'recommends': recommends,
                            'engagement': hits + recommends * 10
                        })

                        successful_posts += 1

                    except Exception as e:
                        continue

                if len(posts) > 0:
                    print(f"   âœ… í˜„ì¬ê¹Œì§€ ì´ {len(posts)}ê°œ ê²Œì‹œë¬¼ ìˆ˜ì§‘")

                # Rate limit ë°©ì§€
                time.sleep(random.uniform(1, 2))

            except Exception as e:
                print(f"   âš ï¸ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                continue

        print(f"\nğŸ“Š ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(posts)}ê°œ ê²Œì‹œë¬¼")
        return posts

    def extract_keywords_from_posts(self, posts: List[Dict],
                                   min_length: int = 2) -> List[Dict]:
        """
        ê²Œì‹œë¬¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ

        Args:
            posts: ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸
            min_length: ìµœì†Œ í‚¤ì›Œë“œ ê¸¸ì´

        Returns:
            í‚¤ì›Œë“œì™€ ë¹ˆë„ìˆ˜
        """
        keyword_counter = Counter()
        keyword_engagement = {}

        # ë¶ˆìš©ì–´
        stopwords = {
            'ë½ë¿Œ', 'ê²Œì‹œíŒ', 'ê²Œì‹œê¸€', 'ê³µì§€', 'ì§ˆë¬¸', 'ë‹µë³€',
            'ì…ë‹ˆë‹¤', 'í•©ë‹ˆë‹¤', 'ìˆìŠµë‹ˆë‹¤', 'ì—†ìŠµë‹ˆë‹¤', 'ê°€ëŠ¥', 'ë¶ˆê°€ëŠ¥',
            'ì´ê±°', 'ì €ê±°', 'ê·¸ê±°', 'ì´ê²Œ', 'ì €ê²Œ', 'ê·¸ê²Œ',
            'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ìš”ì¦˜', 'ì§€ê¸ˆ', 'ì´ì œ', 'ê·¸ëƒ¥',
            'ì§„ì§œ', 'ì •ë§', 'ì™„ì „', 'ë„ˆë¬´', 'ì—„ì²­', 'ê°œ', 'ë§¤ìš°',
            'ìˆë‹¤', 'ì—†ë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ì´ë‹¤', 'ì•„ë‹ˆë‹¤',
            'ê°™ë‹¤', 'ë“¯í•˜ë‹¤', 'ë³´ì´ë‹¤', 'ì‹¶ë‹¤', 'í•˜ê³ ', 'ê·¸ë¦¬ê³ ',
            'ë˜ëŠ”', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ê·¸ë˜ì„œ', 'ë•Œë¬¸ì—',
            'ì•ˆë…•í•˜ì„¸ìš”', 'ê°ì‚¬í•©ë‹ˆë‹¤', 'ìˆ˜ê³ í•˜ì„¸ìš”', 'ë¶€íƒë“œë¦½ë‹ˆë‹¤',
            'í•«ë”œ', 'íŠ¹ê°€', 'í• ì¸', 'ìµœì €ê°€', 'ë¬´ë£Œë°°ì†¡', 'ì¿ í°'
        }

        for post in posts:
            title = post['title']
            engagement = post.get('engagement', 1)

            # í•œê¸€ í‚¤ì›Œë“œ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
            korean_words = re.findall(r'[ê°€-í£]{2,}', title)

            # ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
            english_words = re.findall(r'\b[A-Za-z]{2,}\b', title)

            # ìˆ«ì+í…ìŠ¤íŠ¸ ì¡°í•© (ê°€ê²©, ë‚ ì§œ ë“±)
            mixed_words = re.findall(r'\d+[ê°€-í£]+', title)

            all_words = korean_words + english_words + mixed_words

            for word in all_words:
                # ë¶ˆìš©ì–´ ì œê±°
                if word.lower() in stopwords or len(word) < min_length:
                    continue

                keyword_counter[word] += 1

                # ì¸ê¸°ë„ ëˆ„ì 
                if word not in keyword_engagement:
                    keyword_engagement[word] = 0
                keyword_engagement[word] += engagement

        # ê²°ê³¼ ì •ë¦¬
        keywords = []
        for word, count in keyword_counter.most_common(100):
            keywords.append({
                'keyword': word,
                'count': count,
                'total_engagement': keyword_engagement.get(word, 0),
                'avg_engagement': keyword_engagement.get(word, 0) / count if count > 0 else 0
            })

        return keywords


class PpomppuTrendAnalyzer:
    """ë½ë¿Œ íŠ¸ë Œë“œ ë¶„ì„ê¸°"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.crawler = PpomppuCrawler()

    def analyze_hotdeal(self, max_pages: int = 10) -> Dict:
        """
        í•«ë”œ ê²Œì‹œíŒ ë¶„ì„

        Args:
            max_pages: í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜

        Returns:
            ë¶„ì„ ê²°ê³¼
        """
        print(f"\n{'='*60}")
        print(f"ğŸ”¥ ë½ë¿Œ í•«ë”œ ê²Œì‹œíŒ ë¶„ì„")
        print(f"{'='*60}")

        posts = self.crawler.get_hotdeal_posts(max_pages)

        if not posts:
            print(f"âš ï¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return {}

        print(f"ğŸ“Š ì´ {len(posts)}ê°œ ê²Œì‹œë¬¼ ìˆ˜ì§‘ ì™„ë£Œ")

        # í‚¤ì›Œë“œ ì¶”ì¶œ
        print(f"ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        keywords = self.crawler.extract_keywords_from_posts(posts)

        print(f"âœ… {len(keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ")

        return {
            'source': 'ë½ë¿Œ í•«ë”œ',
            'total_posts': len(posts),
            'keywords': keywords,
            'crawled_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

    def save_results(self, results: Dict, filename: str = 'ppomppu_trends.json'):
        """ê²°ê³¼ ì €ì¥ (JSON)"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def save_results_to_csv(self, results: Dict, filename: str = 'ppomppu_trends.csv'):
        """ê²°ê³¼ ì €ì¥ (CSV)"""
        with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['ê²Œì‹œíŒ', 'ìˆœìœ„', 'í‚¤ì›Œë“œ', 'ì¶œí˜„íšŸìˆ˜', 'ì´_ì¸ê¸°ë„', 'í‰ê· _ì¸ê¸°ë„'])

            for key, result in results.items():
                source_name = result.get('source', key)
                for i, kw in enumerate(result['keywords'], 1):
                    writer.writerow([
                        source_name,
                        i,
                        kw['keyword'],
                        kw['count'],
                        kw.get('total_engagement', 0),
                        round(kw.get('avg_engagement', 0), 2)
                    ])

        print(f"ğŸ’¾ CSV ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# ì‹¤í–‰
if __name__ == "__main__":
    print("\n" + "="*80)
    print("ğŸš€ ë½ë¿Œ íŠ¸ë Œë“œ í¬ë¡¤ë§")
    print("="*80)
    print("\nğŸ›’ ë½ë¿Œì—ì„œ ì‡¼í•‘/í•«ë”œ íŠ¸ë Œë“œ í‚¤ì›Œë“œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    print("âš ï¸  í¬ë¡¤ë§ ì†ë„ ì œí•œì„ ì¤€ìˆ˜í•˜ë©°, ê³µê°œ ê²Œì‹œíŒë§Œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n")

    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = PpomppuTrendAnalyzer()

    try:
        # í•«ë”œ ê²Œì‹œíŒ ë¶„ì„
        print("âœ… í•«ë”œ ê²Œì‹œíŒ ë¶„ì„ ëª¨ë“œ")

        result = analyzer.analyze_hotdeal(max_pages=10)

        if result:
            results = {'hotdeal': result}

            # ê²°ê³¼ ì €ì¥
            analyzer.save_results(results, 'ppomppu_trends_2025.json')
            analyzer.save_results_to_csv(results, 'ppomppu_trends_2025.csv')

            # Top 20 í‚¤ì›Œë“œ ì¶œë ¥
            print("\n" + "="*80)
            print("ğŸ”¥ ë½ë¿Œ í•«ë”œ Top 20 í‚¤ì›Œë“œ")
            print("="*80)

            keywords = result['keywords'][:20]
            for i, kw in enumerate(keywords, 1):
                print(f"{i:2d}. {kw['keyword']:20s} | "
                      f"ì¶œí˜„: {kw['count']:3d}íšŒ | "
                      f"ì¸ê¸°ë„: {kw['total_engagement']:6d}")

            print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ!")
            print(f"ğŸ“… ìˆ˜ì§‘ ì‹œê°„: {result['crawled_at']}")

        if not result:
            print("\nâŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

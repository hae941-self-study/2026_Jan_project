"""
ë””ì‹œì¸ì‚¬ì´ë“œ (DC Inside) íŠ¸ë Œë“œ í¬ë¡¤ë§
- ì¸ê¸° ê°¤ëŸ¬ë¦¬ì˜ ê²Œì‹œë¬¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
- ì œëª©, ë‚´ìš©ì—ì„œ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ë¶„ì„
"""

import requests
from bs4 import BeautifulSoup
import time
import random
import re
from collections import Counter
from typing import List, Dict
import json
import csv
from datetime import datetime
import sys
import io

# Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')


class DCInsideCrawler:
    """ë””ì‹œì¸ì‚¬ì´ë“œ í¬ë¡¤ëŸ¬"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        self.base_url = 'https://gall.dcinside.com'

    def get_gallery_list(self, gallery_id: str, page: int = 1) -> List[Dict]:
        """
        íŠ¹ì • ê°¤ëŸ¬ë¦¬ì˜ ê²Œì‹œë¬¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°

        Args:
            gallery_id: ê°¤ëŸ¬ë¦¬ ID (ì˜ˆ: 'book' - ë„ì„œ ê°¤ëŸ¬ë¦¬)
            page: í˜ì´ì§€ ë²ˆí˜¸

        Returns:
            ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸
        """
        url = f'{self.base_url}/board/lists/?id={gallery_id}&page={page}'

        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'

            soup = BeautifulSoup(response.text, 'html.parser')
            posts = []

            # ê²Œì‹œë¬¼ ëª©ë¡ íŒŒì‹±
            post_list = soup.select('.gall_list tbody tr.ub-content')

            for post in post_list:
                try:
                    # ì œëª©
                    title_elem = post.select_one('.gall_tit a')
                    if not title_elem:
                        continue

                    title = title_elem.text.strip()

                    # ëŒ“ê¸€ ìˆ˜
                    reply_elem = post.select_one('.gall_tit .reply_num')
                    reply_count = 0
                    if reply_elem:
                        reply_text = reply_elem.text.strip()
                        reply_match = re.search(r'\[(\d+)\]', reply_text)
                        if reply_match:
                            reply_count = int(reply_match.group(1))

                    # ì¡°íšŒìˆ˜
                    views_elem = post.select_one('.gall_count')
                    views = 0
                    if views_elem:
                        views_text = views_elem.text.strip()
                        views = int(views_text) if views_text.isdigit() else 0

                    # ì¶”ì²œìˆ˜
                    recommend_elem = post.select_one('.gall_recommend')
                    recommend = 0
                    if recommend_elem:
                        recommend_text = recommend_elem.text.strip()
                        recommend = int(recommend_text) if recommend_text.isdigit() else 0

                    # ì‘ì„±ì¼
                    date_elem = post.select_one('.gall_date')
                    date = date_elem.text.strip() if date_elem else ''

                    posts.append({
                        'title': title,
                        'reply_count': reply_count,
                        'views': views,
                        'recommend': recommend,
                        'date': date,
                        'engagement': reply_count + recommend  # ì¸ê¸°ë„ ì§€í‘œ
                    })

                except Exception as e:
                    continue

            return posts

        except requests.exceptions.RequestException as e:
            print(f"âŒ ê°¤ëŸ¬ë¦¬ ì¡°íšŒ ì‹¤íŒ¨ ({gallery_id}): {e}")
            return []

    def extract_keywords_from_posts(self, posts: List[Dict],
                                   min_length: int = 2) -> List[Dict]:
        """
        ê²Œì‹œë¬¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ

        Args:
            posts: ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸
            min_length: ìµœì†Œ í‚¤ì›Œë“œ ê¸¸ì´

        Returns:
            í‚¤ì›Œë“œì™€ ë¹ˆë„ìˆ˜
        """
        keyword_counter = Counter()
        keyword_engagement = {}  # í‚¤ì›Œë“œë³„ ì¸ê¸°ë„ í•©ì‚°

        # ì¼ë°˜ì ì¸ ë‹¨ì–´ í•„í„°ë§
        stopwords = {
            'ê²Œì‹œíŒ', 'ê°¤ëŸ¬ë¦¬', 'ë””ì‹œì¸ì‚¬ì´ë“œ', 'ë””ì‹œ', 'ì§ˆë¬¸', 'ë‹µë³€',
            'ì…ë‹ˆë‹¤', 'í•©ë‹ˆë‹¤', 'ìˆìŠµë‹ˆë‹¤', 'ì—†ìŠµë‹ˆë‹¤', 'ê°€ëŠ¥', 'ë¶ˆê°€ëŠ¥',
            'ì´ê±°', 'ì €ê±°', 'ê·¸ê±°', 'ì´ê²Œ', 'ì €ê²Œ', 'ê·¸ê²Œ',
            'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ìš”ì¦˜', 'ì§€ê¸ˆ', 'ì´ì œ', 'ê·¸ëƒ¥',
            'ì§„ì§œ', 'ì •ë§', 'ì™„ì „', 'ë„ˆë¬´', 'ì—„ì²­', 'ê°œ', 'ë§¤ìš°',
            'ìˆë‹¤', 'ì—†ë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ì´ë‹¤', 'ì•„ë‹ˆë‹¤',
            'ê°™ë‹¤', 'ë“¯í•˜ë‹¤', 'ë³´ì´ë‹¤', 'ì‹¶ë‹¤', 'í•˜ê³ ', 'ê·¸ë¦¬ê³ ',
            'ë˜ëŠ”', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ê·¸ë˜ì„œ', 'ë•Œë¬¸ì—'
        }

        for post in posts:
            title = post['title']
            engagement = post['engagement']

            # í•œê¸€ í‚¤ì›Œë“œ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
            korean_words = re.findall(r'[ê°€-í£]{2,}', title)

            # ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ (3ê¸€ì ì´ìƒ)
            english_words = re.findall(r'\b[A-Za-z]{3,}\b', title)

            # ìˆ«ì+í…ìŠ¤íŠ¸ ì¡°í•© (ì˜ˆ: 2024ë…„, 3ì›”)
            mixed_words = re.findall(r'\d+[ê°€-í£]+', title)

            all_words = korean_words + english_words + mixed_words

            for word in all_words:
                # ë¶ˆìš©ì–´ ì œê±°
                if word.lower() in stopwords or len(word) < min_length:
                    continue

                keyword_counter[word] += 1

                # ì¸ê¸°ë„ ëˆ„ì 
                if word not in keyword_engagement:
                    keyword_engagement[word] = 0
                keyword_engagement[word] += engagement

        # ê²°ê³¼ ì •ë¦¬
        keywords = []
        for word, count in keyword_counter.most_common(100):
            keywords.append({
                'keyword': word,
                'count': count,
                'total_engagement': keyword_engagement.get(word, 0),
                'avg_engagement': keyword_engagement.get(word, 0) / count if count > 0 else 0
            })

        return keywords

    def crawl_gallery(self, gallery_id: str, gallery_name: str,
                     max_pages: int = 5) -> Dict:
        """
        ê°¤ëŸ¬ë¦¬ í¬ë¡¤ë§

        Args:
            gallery_id: ê°¤ëŸ¬ë¦¬ ID
            gallery_name: ê°¤ëŸ¬ë¦¬ ì´ë¦„
            max_pages: í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜

        Returns:
            í¬ë¡¤ë§ ê²°ê³¼
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“± {gallery_name} ({gallery_id}) í¬ë¡¤ë§ ì¤‘...")
        print(f"{'='*60}")

        all_posts = []

        for page in range(1, max_pages + 1):
            print(f"   í˜ì´ì§€ {page}/{max_pages} ìˆ˜ì§‘ ì¤‘...")

            posts = self.get_gallery_list(gallery_id, page)
            all_posts.extend(posts)

            print(f"   âœ… {len(posts)}ê°œ ê²Œì‹œë¬¼ ìˆ˜ì§‘")

            # Rate limit ë°©ì§€
            time.sleep(random.uniform(1, 2))

        print(f"\nğŸ“Š ì´ {len(all_posts)}ê°œ ê²Œì‹œë¬¼ ìˆ˜ì§‘ ì™„ë£Œ")

        # í‚¤ì›Œë“œ ì¶”ì¶œ
        print(f"ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        keywords = self.extract_keywords_from_posts(all_posts)

        print(f"âœ… {len(keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ")

        return {
            'gallery_id': gallery_id,
            'gallery_name': gallery_name,
            'total_posts': len(all_posts),
            'keywords': keywords,
            'crawled_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }


class DCInsideTrendAnalyzer:
    """ë””ì‹œì¸ì‚¬ì´ë“œ íŠ¸ë Œë“œ ë¶„ì„ê¸°"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.crawler = DCInsideCrawler()

    def analyze_multiple_galleries(self, galleries: List[Dict],
                                   max_pages: int = 5) -> Dict:
        """
        ì—¬ëŸ¬ ê°¤ëŸ¬ë¦¬ ë¶„ì„

        Args:
            galleries: [{'id': 'gallery_id', 'name': 'gallery_name'}, ...]
            max_pages: ê°¤ëŸ¬ë¦¬ë‹¹ í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜

        Returns:
            ì „ì²´ ë¶„ì„ ê²°ê³¼
        """
        results = {}

        for gallery in galleries:
            gallery_id = gallery['id']
            gallery_name = gallery['name']

            result = self.crawler.crawl_gallery(gallery_id, gallery_name, max_pages)
            results[gallery_id] = result

            # ê° ê°¤ëŸ¬ë¦¬ë³„ Top 10 ì¶œë ¥
            print(f"\nğŸ† {gallery_name} Top 10 í‚¤ì›Œë“œ:")
            print("-" * 70)
            for i, kw in enumerate(result['keywords'][:10], 1):
                print(f"{i:2d}. {kw['keyword']:20s} | "
                      f"ì¶œí˜„: {kw['count']:3d}íšŒ | "
                      f"ì¸ê¸°ë„: {kw['total_engagement']:5d}")

            # ê°¤ëŸ¬ë¦¬ ì‚¬ì´ ëŒ€ê¸°
            time.sleep(random.uniform(2, 3))

        return results

    def get_overall_trends(self, results: Dict, top_n: int = 30) -> List[Dict]:
        """
        ì „ì²´ ê°¤ëŸ¬ë¦¬ì—ì„œ í†µí•© íŠ¸ë Œë“œ ì¶”ì¶œ

        Args:
            results: ê°¤ëŸ¬ë¦¬ë³„ ë¶„ì„ ê²°ê³¼
            top_n: ìƒìœ„ Nê°œ í‚¤ì›Œë“œ

        Returns:
            í†µí•© íŠ¸ë Œë“œ í‚¤ì›Œë“œ
        """
        all_keywords = Counter()
        keyword_engagement = {}

        for gallery_id, result in results.items():
            for kw in result['keywords']:
                keyword = kw['keyword']
                count = kw['count']
                engagement = kw['total_engagement']

                all_keywords[keyword] += count

                if keyword not in keyword_engagement:
                    keyword_engagement[keyword] = 0
                keyword_engagement[keyword] += engagement

        # ê²°ê³¼ ì •ë¦¬
        overall_trends = []
        for keyword, count in all_keywords.most_common(top_n):
            overall_trends.append({
                'keyword': keyword,
                'count': count,
                'total_engagement': keyword_engagement[keyword],
                'avg_engagement': keyword_engagement[keyword] / count if count > 0 else 0
            })

        return overall_trends

    def save_results(self, results: Dict, filename: str = 'dcinside_trends.json'):
        """ê²°ê³¼ ì €ì¥ (JSON)"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def save_results_to_csv(self, results: Dict, filename: str = 'dcinside_trends.csv'):
        """ê²°ê³¼ ì €ì¥ (CSV)"""
        with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['ê°¤ëŸ¬ë¦¬', 'ìˆœìœ„', 'í‚¤ì›Œë“œ', 'ì¶œí˜„íšŸìˆ˜', 'ì´_ì¸ê¸°ë„', 'í‰ê· _ì¸ê¸°ë„'])

            for gallery_id, result in results.items():
                gallery_name = result['gallery_name']
                for i, kw in enumerate(result['keywords'], 1):
                    writer.writerow([
                        gallery_name,
                        i,
                        kw['keyword'],
                        kw['count'],
                        kw['total_engagement'],
                        round(kw['avg_engagement'], 2)
                    ])

        print(f"ğŸ’¾ CSV ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# ì‹¤í–‰
if __name__ == "__main__":
    print("\n" + "="*80)
    print("ğŸš€ ë””ì‹œì¸ì‚¬ì´ë“œ íŠ¸ë Œë“œ í¬ë¡¤ë§")
    print("="*80)
    print("\nğŸ“± í•œêµ­ ìµœëŒ€ ì»¤ë®¤ë‹ˆí‹° ë””ì‹œì¸ì‚¬ì´ë“œì—ì„œ ì‹¤ì‹œê°„ íŠ¸ë Œë“œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    print("âš ï¸  í¬ë¡¤ë§ ì†ë„ ì œí•œì„ ì¤€ìˆ˜í•˜ë©°, ê³µê°œ ê²Œì‹œíŒë§Œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n")

    # í¬ë¡¤ë§í•  ê°¤ëŸ¬ë¦¬ ëª©ë¡ (í•„ìš”ì— ë”°ë¼ ìˆ˜ì • ê°€ëŠ¥)
    galleries = [
        {'id': 'book', 'name': 'ë„ì„œ ê°¤ëŸ¬ë¦¬'},
        {'id': 'comic_new2', 'name': 'ë§Œí™” ê°¤ëŸ¬ë¦¬'},
        {'id': 'movie', 'name': 'ì˜í™” ê°¤ëŸ¬ë¦¬'},
        {'id': 'drama', 'name': 'ë“œë¼ë§ˆ ê°¤ëŸ¬ë¦¬'},
        {'id': 'music', 'name': 'ìŒì•… ê°¤ëŸ¬ë¦¬'},
        {'id': 'game', 'name': 'ê²Œì„ ê°¤ëŸ¬ë¦¬'},
    ]

    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = DCInsideTrendAnalyzer()

    try:
        # ê°¤ëŸ¬ë¦¬ë³„ í¬ë¡¤ë§ ë° ë¶„ì„
        results = analyzer.analyze_multiple_galleries(
            galleries=galleries,
            max_pages=5  # ê°¤ëŸ¬ë¦¬ë‹¹ 5í˜ì´ì§€ (ì•½ 50ê°œ ê²Œì‹œë¬¼)
        )

        if results:
            # ê²°ê³¼ ì €ì¥
            analyzer.save_results(results, 'dcinside_trends_2025.json')
            analyzer.save_results_to_csv(results, 'dcinside_trends_2025.csv')

            # ì „ì²´ í†µí•© íŠ¸ë Œë“œ
            print("\n" + "="*80)
            print("ğŸ“Š ì „ì²´ ê°¤ëŸ¬ë¦¬ í†µí•© íŠ¸ë Œë“œ Top 20")
            print("="*80)

            overall_trends = analyzer.get_overall_trends(results, top_n=20)
            for i, kw in enumerate(overall_trends, 1):
                print(f"{i:2d}. {kw['keyword']:20s} | "
                      f"ì¶œí˜„: {kw['count']:3d}íšŒ | "
                      f"ì¸ê¸°ë„: {kw['total_engagement']:6d}")

            # ê°¤ëŸ¬ë¦¬ë³„ ìš”ì•½
            print("\n" + "="*80)
            print("ğŸ“± ê°¤ëŸ¬ë¦¬ë³„ ìš”ì•½")
            print("="*80)
            for gallery_id, result in results.items():
                print(f"\n{result['gallery_name']}:")
                top_5 = result['keywords'][:5]
                for i, kw in enumerate(top_5, 1):
                    print(f"  {i}. {kw['keyword']} (ì¶œí˜„: {kw['count']}íšŒ)")

            print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(results)}ê°œ ê°¤ëŸ¬ë¦¬ ë¶„ì„")
            print(f"ğŸ“… ìˆ˜ì§‘ ì‹œê°„: {results[list(results.keys())[0]]['crawled_at']}")

        else:
            print("\nâŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
